{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:47:45.579746Z","iopub.status.busy":"2024-05-15T09:47:45.579430Z","iopub.status.idle":"2024-05-15T09:47:45.587429Z","shell.execute_reply":"2024-05-15T09:47:45.585816Z","shell.execute_reply.started":"2024-05-15T09:47:45.579709Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","np.random.seed(10)\n","\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["# Library Functions"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:47:46.257776Z","iopub.status.busy":"2024-05-15T09:47:46.257458Z","iopub.status.idle":"2024-05-15T09:47:46.277304Z","shell.execute_reply":"2024-05-15T09:47:46.275761Z","shell.execute_reply.started":"2024-05-15T09:47:46.257724Z"},"trusted":true},"outputs":[],"source":["def accuracy_score1(y_true, y_pred):\n","    \n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","    \n","    # Check if lengths match\n","    assert len(y_true) == len(y_pred), \"True and predicted labels must have the same length.\"\n","    \n","    correct_predictions = np.sum(y_true == y_pred)\n","    \n","    accuracy = correct_predictions / len(y_true)\n","    \n","    return accuracy\n","\n","def confusion_matrix1(y_true, y_pred):\n","\n","    # Convert lists to arrays if they aren't already\n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","    \n","    cm = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))))\n","    \n","    # Fill confusion matrix\n","    for i, true_label in enumerate(np.unique(y_true)):\n","        for j, pred_label in enumerate(np.unique(y_pred)):\n","            cm[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n","    \n","    return cm\n","def train_test_split1(features, labels, test_size=0.2, shuffle=True):\n","    num_samples = features.shape[0]\n","    num_test_samples = int(num_samples * test_size)\n","    \n","    # Shuffle the data if requested\n","    if shuffle:\n","        indices = np.arange(num_samples)\n","        np.random.shuffle(indices)\n","        features = features[indices]\n","        labels = labels[indices]\n","    \n","    # Split the data\n","    test_indices = np.arange(num_test_samples)\n","    train_indices = np.arange(num_samples)[num_test_samples:]\n","    \n","    # Extract the training and testing datasets\n","    X_train = features[train_indices]\n","    X_test = features[test_indices]\n","    Y_train = labels[train_indices]\n","    Y_test = labels[test_indices]\n","    \n","    return X_train, X_test, Y_train, Y_test\n","\n","def calculate_metrics(y_true, y_pred):\n","    if isinstance(y_true, pd.Series):\n","        y_true = y_true.values\n","    if isinstance(y_pred, pd.Series):\n","        y_pred = y_pred.values\n","    \n","    # Confusion Matrix\n","    tp = np.sum((y_true == 1) & (y_pred == 1))\n","    tn = np.sum((y_true == 0) & (y_pred == 0))\n","    fp = np.sum((y_true == 0) & (y_pred == 1))\n","    fn = np.sum((y_true == 1) & (y_pred == 0))\n","    \n","    # Accuracy\n","    accuracy = (tp + tn) / (tp + tn + fp + fn)\n","    \n","    # Precision\n","    if tp + fp == 0:\n","        precision = 0\n","    else:\n","        precision = tp / (tp + fp)\n","    \n","    # Recall\n","    if tp + fn == 0:\n","        recall = 0\n","    else:\n","        recall = tp / (tp + fn)\n","    \n","    # F1 Score\n","    if precision + recall == 0:\n","        f1_score = 0\n","    else:\n","        f1_score = 2 * (precision * recall) / (precision + recall)\n","    \n","    return {\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1 Score': f1_score\n","    }\n"]},{"cell_type":"markdown","metadata":{},"source":["# The Data\n","\n","Since this is somewhat of a tutorial, we will use a very basic Multi-Layer Perceptron to predict if a set of passengers survived; as a feature, we will only use the *Passenger Class* for sake of simplicity. We then split the set of features and labels into a training set and a test set."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:47:47.197847Z","iopub.status.busy":"2024-05-15T09:47:47.197129Z","iopub.status.idle":"2024-05-15T09:47:47.223740Z","shell.execute_reply":"2024-05-15T09:47:47.222553Z","shell.execute_reply.started":"2024-05-15T09:47:47.197389Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training records: 624\n","Test records: 267\n"]}],"source":["data = pd.read_csv('train.csv')\n","# We define a dictionary to transform the 0,1 values in the labels to a String that defines the fate of the passenger\n","dict_live = { \n","    0 : 'Perished',\n","    1 : 'Survived'\n","}\n","\n","# We define a dictionary to binarize the sex\n","dict_sex = {\n","    'male' : 0,\n","    'female' : 1\n","}\n","\n","# We apply the dictionary using a lambda function and the pandas .apply() module\n","data['Bsex'] = data['Sex'].apply(lambda x : dict_sex[x])\n","\n","\n","# Now the features are a 2 column matrix whose entries are the Class (1,2,3) and the Sex (0,1) of the passengers\n","features = data[['Pclass', 'Bsex']].to_numpy()\n","labels = data['Survived'].to_numpy()\n","\n","# split into training and testing sets\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, Y_train, Y_test = train_test_split1(features, labels, test_size=0.30)\n","\n","print('Training records:',Y_train.size)\n","print('Test records:',Y_test.size)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:47:48.021125Z","iopub.status.busy":"2024-05-15T09:47:48.020742Z","iopub.status.idle":"2024-05-15T09:47:48.028599Z","shell.execute_reply":"2024-05-15T09:47:48.026840Z","shell.execute_reply.started":"2024-05-15T09:47:48.021089Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[3, 0],\n","       [1, 1],\n","       [3, 1],\n","       ...,\n","       [3, 1],\n","       [1, 0],\n","       [3, 0]])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["features"]},{"cell_type":"markdown","metadata":{},"source":["\n","In the **Activation_function** class we have\n","* **ReLU_act, sigmoid_act**: these are the activation functions. They can be easily generalized (LeakyReLU, ParametricReLU, etc.)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:47:48.974116Z","iopub.status.busy":"2024-05-15T09:47:48.973661Z","iopub.status.idle":"2024-05-15T09:47:48.983379Z","shell.execute_reply":"2024-05-15T09:47:48.982104Z","shell.execute_reply.started":"2024-05-15T09:47:48.974039Z"},"trusted":true},"outputs":[],"source":["def sigmoid_act(x, der=False):\n","    if (der==True) : #derivative of the sigmoid\n","        f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n","    else : # sigmoid\n","        f = 1/(1+ np.exp(- x))\n","    return f\n","\n","def ReLU_act(x, der=False):\n","    if (der == True): # the derivative of the ReLU is the Heaviside Theta\n","        f = np.heaviside(x, 1)\n","    else :\n","        f = np.maximum(x, 0)\n","    return f"]},{"cell_type":"markdown","metadata":{},"source":["In the **layers** class we have\n","* **layer**: it eats two imputs, the number of neurons and the activation function (as a string); it returns a tuple of the two. The idea is to leave room for a generalization of the *layers.layer* method later on by adding multiple layer type (i.e. Pooling or Convolutional layers for the CNN)."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:47:49.923622Z","iopub.status.busy":"2024-05-15T09:47:49.923348Z","iopub.status.idle":"2024-05-15T09:47:49.928684Z","shell.execute_reply":"2024-05-15T09:47:49.927562Z","shell.execute_reply.started":"2024-05-15T09:47:49.923590Z"},"trusted":true},"outputs":[],"source":["class layers :     \n","    def layer(p=4, activation = 'ReLU'):\n","        return (p, activation)"]},{"cell_type":"markdown","metadata":{},"source":["# The ANN as a Class\n","\n","We want now to implement the code as Class for Python, so that we could easiliy generalize it; the goal is to have a class from which we can instantiate an object \"Neural Network\", and add to it as many hidden layers with as many neurons we want, with the desired activation functions and so on. To generalize further, we define also the *Activation_function* class and the *layers* class, so we may easily add more activation funtions or more different layers (such as Convolutional or Pooling layers for Convolutional Neural Networks).\n","\n","Within the ANN class, we define the following methods:\n","* **add**: it eats a tuple ( int(number_of_neurons), string(activation_function) ), i.e. the output of the ANN.layer method. It is a void method. It updates the HiddenLayer string defined by the __init__ method. \n","* **FeedForward**: it implements the Feed Forward layer by layer.\n","* **BackPropagation**: it implements the whole gradient descent mechanism; first, it computes the errors by implementing the backpropagation; then, it updates the ANN parameters by gradient descent. \n","* **Fit**: this method eats the training features and labels and fits the ANN by calling iteratively *FeedForward* and *BackPropagation* methods. This allow us to easily modify (or generalize) either *FeedForward* or *BackPropagation* methods without altering the *Fit* method.\n","* **predict**: it eats the featurs and spits the label predictions. \n","* **set_learning_rate**: by default the learning rate is initialized to be 1, but we can call this method to set it to a different value. \n","* **get_accuracy, get_avg_accuracy**: these methods' aim is to return the cost function either at each step of the training process or averaging over 10 passengers, respectively.\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:55:21.565356Z","iopub.status.busy":"2024-05-15T09:55:21.564865Z","iopub.status.idle":"2024-05-15T09:55:21.627839Z","shell.execute_reply":"2024-05-15T09:55:21.625814Z","shell.execute_reply.started":"2024-05-15T09:55:21.565268Z"},"trusted":true},"outputs":[],"source":["class ANN:\n","    np.random.seed(10)\n","    \n","    '''\n","    HiddenLayer vector : will contain the Layers' info\n","    w, b, phi = (empty) arrays that will contain all the w, b and activation functions for all the Layers\n","    mu = cost function\n","    eta = a standard learning rate initialization. It can be modified by the 'set_learning_rate' method\n","    '''\n","    def __init__(self) :\n","        self.HiddenLayer = []\n","        self.w = []\n","        self.b = []\n","        self.phi = []\n","        self.mu = []\n","        self.el = []\n","        self.eta = 1 #set up the proper Learning Rate!!\n","        self.loss_name = 'mse'\n","\n","    def add(self, lay = (4, 'ReLU') ):\n","        self.HiddenLayer.append(lay)\n","\n","    @staticmethod\n","    def FeedForward(w, b, phi, x):\n","        return phi(np.dot(w, x) + b)\n","\n","    def BackPropagation(self, x, z, Y, w, b, phi):\n","        self.delta = []\n","        \n","        # We initialize ausiliar w and b that are used only inside the backpropagation algorithm once called        \n","        self.W = []\n","        self.B = []\n","        \n","        # We start computing the LAST error, the one for the OutPut Layer \n","        self.delta.append(  (z[len(z)-1] - Y) * phi[len(z)-1](z[len(z)-1], der=True) )\n","        \n","        '''Now we BACKpropagate'''\n","        # We thus compute from next-to-last to first\n","        for i in range(0, len(z)-1):\n","            self.delta.append( np.dot( self.delta[i], w[len(z)- 1 - i] ) * phi[len(z)- 2 - i](z[len(z)- 2 - i], der=True) )\n","        \n","        # print(self.delta, w, b,self.X.shape[0])\n","        # We have the error array ordered from last to first; we flip it to order it from first to last\n","        self.delta = np.flip(self.delta, 0)  \n","        \n","        # Now we define the delta as the error divided by the number of training samples\n","        self.delta = self.delta/self.X.shape[0] \n","        \n","        '''GRADIENT DESCENT'''\n","        # We start from the first layer that is special, since it is connected to the Input Layer\n","        self.W.append( w[0] - self.eta * np.kron(self.delta[0], x).reshape( len(z[0]), x.shape[0] ) )\n","        self.B.append( b[0] - self.eta * self.delta[0] )\n","        \n","        # We now descend for all the other Hidden Layers + OutPut Layer\n","        for i in range(1, len(z)):\n","            self.W.append( w[i] - self.eta * np.kron(self.delta[i], z[i-1]).reshape(len(z[i]), len(z[i-1])) )\n","            self.B.append( b[i] - self.eta * self.delta[i] )\n","        \n","        # We return the descended parameters w, b\n","        return np.array(self.W), np.array(self.B)\n","    \n","    def Fit(self, X_train, Y_train, num_epochs = 1):            \n","        print('Start fitting...')\n","        '''\n","        Input layer\n","        '''\n","        self.X = X_train\n","        self.Y = Y_train\n","        \n","        '''\n","        We now initialize the Network by retrieving the Hidden Layers and concatenating them \n","        '''\n","        print('Model recap: \\n')\n","        print('You are fitting an ANN with the following amount of layers: ', len(self.HiddenLayer))\n","        \n","        for i in range(0, len(self.HiddenLayer)) :\n","            print('Layer ', i+1)\n","            print('Number of neurons: ', self.HiddenLayer[i][0])\n","            if i==0:\n","                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.X.shape[1])/np.sqrt(2/self.X.shape[1]) )\n","                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.X.shape[1]))\n","                \n","                # Initialize the Activation function\n","                for act in Activation_function.list_act():\n","                    if self.HiddenLayer[i][1] == act :\n","                        self.phi.append(Activation_function.get_act(act))\n","                        print('\\tActivation: ', act)\n","\n","            else :\n","                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] )/np.sqrt(2/self.HiddenLayer[i-1][0]))\n","                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.HiddenLayer[i-1][0]))\n","\n","                # Initialize the Activation function\n","                for act in Activation_function.list_act():\n","                    if self.HiddenLayer[i][1] == act :\n","                        self.phi.append(Activation_function.get_act(act))\n","                        print('\\tActivation: ', act)\n","        epoch_count = 0 \n","        for epoch in range(num_epochs):\n","            total_loss = 0\n","            for I in range(0, self.X.shape[0]): # loop over the training set\n","\n","                self.z = []\n","            \n","                self.z.append( self.FeedForward(self.w[0], self.b[0], self.phi[0], self.X[I]) ) # First layers\n","            \n","                for i in range(1, len(self.HiddenLayer)): #Looping over layers\n","                    self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1] ) )\n","  \n","                self.w, self.b  = self.BackPropagation(self.X[I], self.z, self.Y[I], self.w, self.b, self.phi)\n","\n","                loss = self.compute_cost(I)\n","                self.mu.append(loss)\n","                total_loss += loss\n","            epoch_count += 1\n","            print(f\"Epoch {epoch_count} completed. Loss: {total_loss / self.X.shape[0]}\")\n","            self.el.append(total_loss/ self.X.shape[0])\n","        print('Fit done. \\n')\n","\n","    def compute_cost(self, I):\n","        if self.loss_name == 'mse':\n","            cost = np.mean((self.z[-1] - self.Y[I])**2)\n","        elif self.loss_name == 'mae':\n","            cost = np.mean(np.abs(self.z[-1] - self.Y[I]))\n","        else:\n","            raise ValueError(\"Invalid loss name\")\n","        return cost\n","    \n","    def predict(self, X_test):\n","        \n","        print('Starting predictions...')\n","        \n","        self.pred = []\n","        self.XX = X_test\n","        \n","        for I in range(0, self.XX.shape[0]):\n","            self.z = []\n","            \n","            self.z.append(self.FeedForward(self.w[0] , self.b[0], self.phi[0], self.XX[I])) #First layer\n","    \n","            for i in range(1, len(self.HiddenLayer)) : # loop over the layers\n","                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1]))\n","       \n","            # Append the prediction;\n","            # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n","            # if y < 0.5 the output is zero, otherwise is zero\n","            self.pred.append( np.heaviside(  self.z[-1] - 0.5, 1)[0] ) # NB: self.z[-1]  is the last element of the self.z list\n","        \n","        print('Predictions done. \\n')\n","\n","        return np.array(self.pred)\n","    def get_losses(self):\n","        return np.array(self.el)\n","    def set_loss_function(self, loss_name):\n","        if loss_name.lower() not in ['mse', 'mae']:\n","            raise ValueError(\"Invalid loss function. Please choose 'mse' or 'mae'.\")\n","        self.loss_name = loss_name.lower()\n","        print('Loss Function is: ',self.loss_name)\n","    def set_learning_rate(self, et=1):\n","        self.eta = et\n","        \n","class Activation_function(ANN):\n","    import numpy as np\n","    \n","    def __init__(self) :\n","        super().__init__()\n","        \n","    def list_act():\n","        return ['sigmoid', 'ReLU']\n","    \n","    def get_act(string = 'ReLU'):\n","        if string == 'ReLU':\n","            return ReLU_act\n","        elif string == 'sigmoid':\n","            return sigmoid_act\n","        else :\n","            return sigmoid_act"]},{"cell_type":"markdown","metadata":{},"source":["# ANN Training & Comparison"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# model = ANN()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:56:05.496876Z","iopub.status.busy":"2024-05-15T09:56:05.496609Z","iopub.status.idle":"2024-05-15T09:56:14.247930Z","shell.execute_reply":"2024-05-15T09:56:14.246696Z","shell.execute_reply.started":"2024-05-15T09:56:05.496837Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Brand New ANN Model Created\n","Loss Function is:  mse\n","Start fitting...\n","Model recap: \n","\n","You are fitting an ANN with the following amount of layers:  4\n","Layer  1\n","Number of neurons:  24\n","\tActivation:  ReLU\n","Layer  2\n","Number of neurons:  24\n","\tActivation:  sigmoid\n","Layer  3\n","Number of neurons:  6\n","\tActivation:  ReLU\n","Layer  4\n","Number of neurons:  1\n","\tActivation:  sigmoid\n"]},{"ename":"TypeError","evalue":"unsupported operand type(s) for /: 'list' and 'int'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mset_learning_rate(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     11\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m el \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_losses()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# acc_avg_val = model.get_avg_accuracy()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print()\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 110\u001b[0m, in \u001b[0;36mANN.Fit\u001b[0;34m(self, X_train, Y_train, num_epochs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mHiddenLayer)): \u001b[38;5;66;03m#Looping over layers\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFeedForward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw[i] , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] ) )\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBackPropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mI\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mI\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_cost(I)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu\u001b[38;5;241m.\u001b[39mappend(loss)\n","Cell \u001b[0;32mIn[12], line 47\u001b[0m, in \u001b[0;36mANN.BackPropagation\u001b[0;34m(self, x, z, Y, w, b, phi)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mappend( np\u001b[38;5;241m.\u001b[39mdot( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta[i], w[\u001b[38;5;28mlen\u001b[39m(z)\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m i] ) \u001b[38;5;241m*\u001b[39m phi[\u001b[38;5;28mlen\u001b[39m(z)\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m i](z[\u001b[38;5;28mlen\u001b[39m(z)\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m i], der\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# print(self.delta, w, b,self.X.shape[0])\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# We have the error array ordered from last to first; we flip it to order it from first to last\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# self.delta = np.flip(self.delta, 0)  \u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Now we define the delta as the error divided by the number of training samples\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''GRADIENT DESCENT'''\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# We start from the first layer that is special, since it is connected to the Input Layer\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"]}],"source":["model = ANN()\n","print('Brand New ANN Model Created')\n","model.add(layers.layer(24, 'ReLU'))\n","model.add(layers.layer(24, 'sigmoid'))\n","model.add(layers.layer(6, 'ReLU'))\n","# model.add(layers.layer(4, 'ReLU'))\n","model.add(layers.layer(1, 'sigmoid'))\n","model.set_loss_function('mse')\n","model.set_learning_rate(0.1)\n","\n","num_epochs = 50\n","model.Fit(X_train, Y_train, num_epochs)\n","el = model.get_losses()\n","# acc_avg_val = model.get_avg_accuracy()\n","# print()\n","plt.figure(figsize=(10,6))\n","plt.scatter(np.arange(1, num_epochs+1), el, label='mu')\n","plt.title('Avg Loss by epoch', fontsize=20)\n","plt.xlabel('Epoch', fontsize=16)\n","plt.ylabel('Loss', fontsize=16)\n","plt.show()\n","\n","predictions = model.predict(X_test)\n","\n","# Plot the confusion matrix\n","cm = confusion_matrix1(Y_test, predictions)\n","print(calculate_metrics(Y_test, predictions))\n","\n","df_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\n","plt.figure(figsize = (7,7))\n","sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n","plt.xlabel(\"Predicted Class\", fontsize=18)\n","plt.ylabel(\"True Class\", fontsize=18)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Submission to test model!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T09:33:50.778619Z","iopub.status.busy":"2024-05-15T09:33:50.778319Z","iopub.status.idle":"2024-05-15T09:33:50.838773Z","shell.execute_reply":"2024-05-15T09:33:50.837483Z","shell.execute_reply.started":"2024-05-15T09:33:50.778576Z"},"trusted":true},"outputs":[],"source":["test_data = pd.read_csv('../input/titanic/test.csv')\n","\n","test_data.head(4)\n","\n","# We apply the dictionary using a lambda function and the pandas .apply() module\n","test_data['Bsex'] = test_data['Sex'].apply(lambda x : dict_sex[x])\n","\n","\n","X_t = test_data[['Pclass', 'Bsex']].to_numpy()\n","\n","test_predictions = model.predict(X_t)\n","submission = pd.DataFrame({\n","        \"PassengerId\": test_data[\"PassengerId\"],\n","        \"Survived\": test_predictions\n","    })\n","\n","\n","submission['Survived'] = submission['Survived'].astype(int)\n","# Export it in a 'Comma Separated Values' (CSV) file\n","import os\n","os.chdir(r'../working')\n","submission.to_csv(r'submission.csv', index=False)\n","# Creating a link to download the .csv file we created\n","from IPython.display import FileLink\n","FileLink(r'submission.csv')\n","# submission.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":26502,"sourceId":3136,"sourceType":"competition"}],"dockerImageVersionId":29844,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
