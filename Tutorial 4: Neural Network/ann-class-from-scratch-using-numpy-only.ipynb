{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:28:52.124535Z",
     "iopub.status.busy": "2024-05-15T09:28:52.124117Z",
     "iopub.status.idle": "2024-05-15T09:28:55.175241Z",
     "shell.execute_reply": "2024-05-15T09:28:55.173544Z",
     "shell.execute_reply.started": "2024-05-15T09:28:52.124491Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:28:58.352196Z",
     "iopub.status.busy": "2024-05-15T09:28:58.351851Z",
     "iopub.status.idle": "2024-05-15T09:28:58.371835Z",
     "shell.execute_reply": "2024-05-15T09:28:58.370121Z",
     "shell.execute_reply.started": "2024-05-15T09:28:58.352141Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_score1(y_true, y_pred):\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Check if lengths match\n",
    "    assert len(y_true) == len(y_pred), \"True and predicted labels must have the same length.\"\n",
    "    \n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    \n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def confusion_matrix1(y_true, y_pred):\n",
    "\n",
    "    # Convert lists to arrays if they aren't already\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    cm = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))))\n",
    "    \n",
    "    # Fill confusion matrix\n",
    "    for i, true_label in enumerate(np.unique(y_true)):\n",
    "        for j, pred_label in enumerate(np.unique(y_pred)):\n",
    "            cm[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
    "    \n",
    "    return cm\n",
    "def train_test_split1(features, labels, test_size=0.2, shuffle=True):\n",
    "    num_samples = features.shape[0]\n",
    "    num_test_samples = int(num_samples * test_size)\n",
    "    \n",
    "    # Shuffle the data if requested\n",
    "    if shuffle:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        features = features[indices]\n",
    "        labels = labels[indices]\n",
    "    \n",
    "    # Split the data\n",
    "    test_indices = np.arange(num_test_samples)\n",
    "    train_indices = np.arange(num_samples)[num_test_samples:]\n",
    "    \n",
    "    # Extract the training and testing datasets\n",
    "    X_train = features[train_indices]\n",
    "    X_test = features[test_indices]\n",
    "    Y_train = labels[train_indices]\n",
    "    Y_test = labels[test_indices]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.Series):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Precision\n",
    "    if tp + fp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    \n",
    "    # Recall\n",
    "    if tp + fn == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    \n",
    "    # F1 Score\n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Since this is somewhat of a tutorial, we will use a very basic Multi-Layer Perceptron to predict if a set of passengers survived; as a feature, we will only use the *Passenger Class* for sake of simplicity. We then split the set of features and labels into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:28:58.740867Z",
     "iopub.status.busy": "2024-05-15T09:28:58.740439Z",
     "iopub.status.idle": "2024-05-15T09:28:59.174904Z",
     "shell.execute_reply": "2024-05-15T09:28:59.173001Z",
     "shell.execute_reply.started": "2024-05-15T09:28:58.740814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training records: 624\n",
      "Test records: 267\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../input/titanic/train.csv')\n",
    "# We define a dictionary to transform the 0,1 values in the labels to a String that defines the fate of the passenger\n",
    "dict_live = { \n",
    "    0 : 'Perished',\n",
    "    1 : 'Survived'\n",
    "}\n",
    "\n",
    "# We define a dictionary to binarize the sex\n",
    "dict_sex = {\n",
    "    'male' : 0,\n",
    "    'female' : 1\n",
    "}\n",
    "\n",
    "# We apply the dictionary using a lambda function and the pandas .apply() module\n",
    "data['Bsex'] = data['Sex'].apply(lambda x : dict_sex[x])\n",
    "\n",
    "\n",
    "# Now the features are a 2 column matrix whose entries are the Class (1,2,3) and the Sex (0,1) of the passengers\n",
    "features = data[['Pclass', 'Bsex']].to_numpy()\n",
    "labels = data['Survived'].to_numpy()\n",
    "\n",
    "# split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split1(features, labels, test_size=0.30)\n",
    "\n",
    "print('Training records:',Y_train.size)\n",
    "print('Test records:',Y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:28:59.180020Z",
     "iopub.status.busy": "2024-05-15T09:28:59.179331Z",
     "iopub.status.idle": "2024-05-15T09:28:59.189778Z",
     "shell.execute_reply": "2024-05-15T09:28:59.188000Z",
     "shell.execute_reply.started": "2024-05-15T09:28:59.179922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0],\n",
       "       [1, 1],\n",
       "       [3, 1],\n",
       "       ...,\n",
       "       [3, 1],\n",
       "       [1, 0],\n",
       "       [3, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the **Activation_function** class we have\n",
    "* **ReLU_act, sigmoid_act**: these are the activation functions. They can be easily generalized (LeakyReLU, ParametricReLU, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:29:00.411953Z",
     "iopub.status.busy": "2024-05-15T09:29:00.411646Z",
     "iopub.status.idle": "2024-05-15T09:29:00.422815Z",
     "shell.execute_reply": "2024-05-15T09:29:00.420881Z",
     "shell.execute_reply.started": "2024-05-15T09:29:00.411909Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_act(x, der=False):\n",
    "    if (der==True) : #derivative of the sigmoid\n",
    "        f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n",
    "    else : # sigmoid\n",
    "        f = 1/(1+ np.exp(- x))\n",
    "    return f\n",
    "\n",
    "def ReLU_act(x, der=False):\n",
    "    if (der == True): # the derivative of the ReLU is the Heaviside Theta\n",
    "        f = np.heaviside(x, 1)\n",
    "    else :\n",
    "        f = np.maximum(x, 0)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **layers** class we have\n",
    "* **layer**: it eats two imputs, the number of neurons and the activation function (as a string); it returns a tuple of the two. The idea is to leave room for a generalization of the *layers.layer* method later on by adding multiple layer type (i.e. Pooling or Convolutional layers for the CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:29:01.690850Z",
     "iopub.status.busy": "2024-05-15T09:29:01.690484Z",
     "iopub.status.idle": "2024-05-15T09:29:01.700169Z",
     "shell.execute_reply": "2024-05-15T09:29:01.696590Z",
     "shell.execute_reply.started": "2024-05-15T09:29:01.690783Z"
    }
   },
   "outputs": [],
   "source": [
    "class layers :     \n",
    "    def layer(p=4, activation = 'ReLU'):\n",
    "        return (p, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ANN as a Class\n",
    "\n",
    "We want now to implement the code as Class for Python, so that we could easiliy generalize it; the goal is to have a class from which we can instantiate an object \"Neural Network\", and add to it as many hidden layers with as many neurons we want, with the desired activation functions and so on. To generalize further, we define also the *Activation_function* class and the *layers* class, so we may easily add more activation funtions or more different layers (such as Convolutional or Pooling layers for Convolutional Neural Networks).\n",
    "\n",
    "Within the ANN class, we define the following methods:\n",
    "* **add**: it eats a tuple ( int(number_of_neurons), string(activation_function) ), i.e. the output of the ANN.layer method. It is a void method. It updates the HiddenLayer string defined by the __init__ method. \n",
    "* **FeedForward**: it implements the Feed Forward layer by layer.\n",
    "* **BackPropagation**: it implements the whole gradient descent mechanism; first, it computes the errors by implementing the backpropagation; then, it updates the ANN parameters by gradient descent. \n",
    "* **Fit**: this method eats the training features and labels and fits the ANN by calling iteratively *FeedForward* and *BackPropagation* methods. This allow us to easily modify (or generalize) either *FeedForward* or *BackPropagation* methods without altering the *Fit* method.\n",
    "* **predict**: it eats the featurs and spits the label predictions. \n",
    "* **set_learning_rate**: by default the learning rate is initialized to be 1, but we can call this method to set it to a different value. \n",
    "* **get_accuracy, get_avg_accuracy**: these methods' aim is to return the cost function either at each step of the training process or averaging over 10 passengers, respectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:29:03.045819Z",
     "iopub.status.busy": "2024-05-15T09:29:03.045518Z",
     "iopub.status.idle": "2024-05-15T09:29:03.178843Z",
     "shell.execute_reply": "2024-05-15T09:29:03.176578Z",
     "shell.execute_reply.started": "2024-05-15T09:29:03.045769Z"
    }
   },
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    np.random.seed(10)\n",
    "    \n",
    "    '''\n",
    "    HiddenLayer vector : will contain the Layers' info\n",
    "    w, b, phi = (empty) arrays that will contain all the w, b and activation functions for all the Layers\n",
    "    mu = cost function\n",
    "    eta = a standard learning rate initialization. It can be modified by the 'set_learning_rate' method\n",
    "    '''\n",
    "    def __init__(self) :\n",
    "        self.HiddenLayer = []\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        self.phi = []\n",
    "        self.mu = []\n",
    "        self.el = []\n",
    "        self.eta = 1 #set up the proper Learning Rate!!\n",
    "        self.loss_name = 'mse'\n",
    "    \n",
    "    '''\n",
    "    add method: to add layers to the network\n",
    "    '''\n",
    "    def add(self, lay = (4, 'ReLU') ):\n",
    "        self.HiddenLayer.append(lay)\n",
    "    \n",
    "    '''\n",
    "    FeedForward method: as explained before. \n",
    "    '''\n",
    "    @staticmethod\n",
    "    def FeedForward(w, b, phi, x):\n",
    "        return phi(np.dot(w, x) + b)\n",
    "        \n",
    "    '''\n",
    "    BackPropagation algorithm implementing the Gradient Descent \n",
    "    '''\n",
    "    def BackPropagation(self, x, z, Y, w, b, phi):\n",
    "        self.delta = []\n",
    "        \n",
    "        # We initialize ausiliar w and b that are used only inside the backpropagation algorithm once called        \n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        \n",
    "        # We start computing the LAST error, the one for the OutPut Layer \n",
    "        self.delta.append(  (z[len(z)-1] - Y) * phi[len(z)-1](z[len(z)-1], der=True) )\n",
    "        \n",
    "        '''Now we BACKpropagate'''\n",
    "        # We thus compute from next-to-last to first\n",
    "        for i in range(0, len(z)-1):\n",
    "            self.delta.append( np.dot( self.delta[i], w[len(z)- 1 - i] ) * phi[len(z)- 2 - i](z[len(z)- 2 - i], der=True) )\n",
    "        \n",
    "        # We have the error array ordered from last to first; we flip it to order it from first to last\n",
    "        self.delta = np.flip(self.delta, 0)  \n",
    "        \n",
    "        # Now we define the delta as the error divided by the number of training samples\n",
    "        self.delta = self.delta/self.X.shape[0] \n",
    "        \n",
    "        '''GRADIENT DESCENT'''\n",
    "        # We start from the first layer that is special, since it is connected to the Input Layer\n",
    "        self.W.append( w[0] - self.eta * np.kron(self.delta[0], x).reshape( len(z[0]), x.shape[0] ) )\n",
    "        self.B.append( b[0] - self.eta * self.delta[0] )\n",
    "        \n",
    "        # We now descend for all the other Hidden Layers + OutPut Layer\n",
    "        for i in range(1, len(z)):\n",
    "            self.W.append( w[i] - self.eta * np.kron(self.delta[i], z[i-1]).reshape(len(z[i]), len(z[i-1])) )\n",
    "            self.B.append( b[i] - self.eta * self.delta[i] )\n",
    "        \n",
    "        # We return the descended parameters w, b\n",
    "        return np.array(self.W), np.array(self.B)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Fit method: it calls FeedForward and Backpropagation methods\n",
    "    '''\n",
    "    def Fit(self, X_train, Y_train, num_epochs = 1):            \n",
    "        print('Start fitting...')\n",
    "        '''\n",
    "        Input layer\n",
    "        '''\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        \n",
    "        '''\n",
    "        We now initialize the Network by retrieving the Hidden Layers and concatenating them \n",
    "        '''\n",
    "        print('Model recap: \\n')\n",
    "        print('You are fitting an ANN with the following amount of layers: ', len(self.HiddenLayer))\n",
    "        \n",
    "        for i in range(0, len(self.HiddenLayer)) :\n",
    "            print('Layer ', i+1)\n",
    "            print('Number of neurons: ', self.HiddenLayer[i][0])\n",
    "            if i==0:\n",
    "                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.X.shape[1])/np.sqrt(2/self.X.shape[1]) )\n",
    "                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.X.shape[1]))\n",
    "                \n",
    "                # Initialize the Activation function\n",
    "                for act in Activation_function.list_act():\n",
    "                    if self.HiddenLayer[i][1] == act :\n",
    "                        self.phi.append(Activation_function.get_act(act))\n",
    "                        print('\\tActivation: ', act)\n",
    "\n",
    "            else :\n",
    "                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] )/np.sqrt(2/self.HiddenLayer[i-1][0]))\n",
    "                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.HiddenLayer[i-1][0]))\n",
    "\n",
    "                # Initialize the Activation function\n",
    "                for act in Activation_function.list_act():\n",
    "                    if self.HiddenLayer[i][1] == act :\n",
    "                        self.phi.append(Activation_function.get_act(act))\n",
    "                        print('\\tActivation: ', act)\n",
    "            \n",
    "        '''\n",
    "        Now we start the Loop over the training dataset\n",
    "        '''\n",
    "        epoch_count = 0 \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for I in range(0, self.X.shape[0]): # loop over the training set\n",
    "                '''\n",
    "                Now we start the feed forward\n",
    "                '''  \n",
    "                self.z = []\n",
    "            \n",
    "                self.z.append( self.FeedForward(self.w[0], self.b[0], self.phi[0], self.X[I]) ) # First layers\n",
    "            \n",
    "                for i in range(1, len(self.HiddenLayer)): #Looping over layers\n",
    "                    self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1] ) )\n",
    "        \n",
    "            \n",
    "                '''\n",
    "                Here we backpropagate\n",
    "                '''      \n",
    "                self.w, self.b  = self.BackPropagation(self.X[I], self.z, self.Y[I], self.w, self.b, self.phi)\n",
    "            \n",
    "                '''\n",
    "                Compute cost function\n",
    "                ''' \n",
    "                \n",
    "                loss = self.compute_cost(I)\n",
    "                self.mu.append(loss)\n",
    "                total_loss += loss\n",
    "            epoch_count += 1\n",
    "            print(f\"Epoch {epoch_count} completed. Loss: {total_loss / self.X.shape[0]}\")\n",
    "            self.el.append(total_loss/ self.X.shape[0])\n",
    "        print('Fit done. \\n')\n",
    "        \n",
    "\n",
    "    \n",
    "    '''\n",
    "    predict method\n",
    "    '''\n",
    "    def compute_cost(self, I):\n",
    "        if self.loss_name == 'mse':\n",
    "            cost = np.mean((self.z[-1] - self.Y[I])**2)\n",
    "        elif self.loss_name == 'mae':\n",
    "            cost = np.mean(np.abs(self.z[-1] - self.Y[I]))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss name\")\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        print('Starting predictions...')\n",
    "        \n",
    "        self.pred = []\n",
    "        self.XX = X_test\n",
    "        \n",
    "        for I in range(0, self.XX.shape[0]): # loop over the training set\n",
    "            \n",
    "            '''\n",
    "            Now we start the feed forward\n",
    "            '''  \n",
    "            self.z = []\n",
    "            \n",
    "            self.z.append(self.FeedForward(self.w[0] , self.b[0], self.phi[0], self.XX[I])) #First layer\n",
    "    \n",
    "            for i in range(1, len(self.HiddenLayer)) : # loop over the layers\n",
    "                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1]))\n",
    "       \n",
    "            # Append the prediction;\n",
    "            # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n",
    "            # if y < 0.5 the output is zero, otherwise is zero\n",
    "            self.pred.append( np.heaviside(  self.z[-1] - 0.5, 1)[0] ) # NB: self.z[-1]  is the last element of the self.z list\n",
    "        \n",
    "        print('Predictions done. \\n')\n",
    "\n",
    "        return np.array(self.pred)\n",
    "    def get_losses(self):\n",
    "        return np.array(self.el)\n",
    "    def set_loss_function(self, loss_name):\n",
    "        if loss_name.lower() not in ['mse', 'mae']:\n",
    "            raise ValueError(\"Invalid loss function. Please choose 'mse' or 'mae'.\")\n",
    "        self.loss_name = loss_name.lower()\n",
    "        print('Loss Function is: ',self.loss_name)\n",
    "    def set_learning_rate(self, et=1):\n",
    "        self.eta = et\n",
    "        \n",
    "class Activation_function(ANN):\n",
    "    import numpy as np\n",
    "    \n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        \n",
    "    def list_act():\n",
    "        return ['sigmoid', 'ReLU']\n",
    "    \n",
    "    def get_act(string = 'ReLU'):\n",
    "        if string == 'ReLU':\n",
    "            return ReLU_act\n",
    "        elif string == 'sigmoid':\n",
    "            return sigmoid_act\n",
    "        else :\n",
    "            return sigmoid_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:36:22.190150Z",
     "iopub.status.busy": "2024-05-15T09:36:22.189805Z",
     "iopub.status.idle": "2024-05-15T09:36:28.620605Z",
     "shell.execute_reply": "2024-05-15T09:36:28.617631Z",
     "shell.execute_reply.started": "2024-05-15T09:36:22.190094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function is:  mse\n",
      "Start fitting...\n",
      "Model recap: \n",
      "\n",
      "You are fitting an ANN with the following amount of layers:  4\n",
      "Layer  1\n",
      "Number of neurons:  24\n",
      "\tActivation:  ReLU\n",
      "Layer  2\n",
      "Number of neurons:  24\n",
      "\tActivation:  sigmoid\n",
      "Layer  3\n",
      "Number of neurons:  6\n",
      "\tActivation:  ReLU\n",
      "Layer  4\n",
      "Number of neurons:  1\n",
      "\tActivation:  sigmoid\n",
      "Epoch 1 completed. Loss: 0.20595596663835622\n",
      "Epoch 2 completed. Loss: 0.18533202306476085\n",
      "Epoch 3 completed. Loss: 0.17852759524252404\n",
      "Epoch 4 completed. Loss: 0.17486622725958448\n",
      "Epoch 5 completed. Loss: 0.1742146954143242\n",
      "Epoch 6 completed. Loss: 0.1743630703770811\n",
      "Epoch 7 completed. Loss: 0.17478753140672287\n",
      "Epoch 8 completed. Loss: 0.17531989470609544\n",
      "Epoch 9 completed. Loss: 0.17584479106977985\n",
      "Epoch 10 completed. Loss: 0.1763527750207287\n",
      "Epoch 11 completed. Loss: 0.1768121456799445\n",
      "Epoch 12 completed. Loss: 0.1772073131742255\n",
      "Epoch 13 completed. Loss: 0.17751764952211305\n",
      "Epoch 14 completed. Loss: 0.17775324849269963\n",
      "Epoch 15 completed. Loss: 0.17792049210704608\n",
      "Epoch 16 completed. Loss: 0.17801812818536034\n",
      "Epoch 17 completed. Loss: 0.17806067571153472\n",
      "Epoch 18 completed. Loss: 0.17807055481893178\n",
      "Epoch 19 completed. Loss: 0.17796286321600022\n",
      "Epoch 20 completed. Loss: 0.17788628236761242\n",
      "Epoch 21 completed. Loss: 0.1778582311711921\n",
      "Epoch 22 completed. Loss: 0.1778479058238259\n",
      "Epoch 23 completed. Loss: 0.17782861138599473\n",
      "Epoch 24 completed. Loss: 0.1777846682093623\n",
      "Epoch 25 completed. Loss: 0.1777018262943413\n",
      "Epoch 26 completed. Loss: 0.17757712481475113\n",
      "Epoch 27 completed. Loss: 0.17724313268558617\n",
      "Epoch 28 completed. Loss: 0.1763678100954793\n",
      "Epoch 29 completed. Loss: 0.17456451921228\n",
      "Epoch 30 completed. Loss: 0.1733176201146545\n",
      "Epoch 31 completed. Loss: 0.17241444092682115\n",
      "Epoch 32 completed. Loss: 0.17180480093482509\n",
      "Epoch 33 completed. Loss: 0.1713878906485205\n",
      "Epoch 34 completed. Loss: 0.17104216465759572\n",
      "Epoch 35 completed. Loss: 0.1707953780010324\n",
      "Epoch 36 completed. Loss: 0.17057189722427035\n",
      "Epoch 37 completed. Loss: 0.17036338522467764\n",
      "Epoch 38 completed. Loss: 0.1701617369694265\n",
      "Epoch 39 completed. Loss: 0.17000933952231387\n",
      "Epoch 40 completed. Loss: 0.1699065822396863\n",
      "Epoch 41 completed. Loss: 0.169837248134915\n",
      "Epoch 42 completed. Loss: 0.16973105507543523\n",
      "Epoch 43 completed. Loss: 0.16963570087460272\n",
      "Epoch 44 completed. Loss: 0.16954822836991593\n",
      "Epoch 45 completed. Loss: 0.16948719511805183\n",
      "Epoch 46 completed. Loss: 0.16951335404050016\n",
      "Epoch 47 completed. Loss: 0.16956389771222125\n",
      "Epoch 48 completed. Loss: 0.16966304160074328\n",
      "Epoch 49 completed. Loss: 0.16971107481711933\n",
      "Epoch 50 completed. Loss: 0.16979013477592989\n",
      "Epoch 51 completed. Loss: 0.1699192708991501\n",
      "Epoch 52 completed. Loss: 0.17002454007273732\n",
      "Epoch 53 completed. Loss: 0.17008345449754886\n",
      "Epoch 54 completed. Loss: 0.1701145719440409\n",
      "Epoch 55 completed. Loss: 0.17010618264039587\n",
      "Epoch 56 completed. Loss: 0.17005016977016926\n",
      "Epoch 57 completed. Loss: 0.1699558752945376\n",
      "Epoch 58 completed. Loss: 0.16984226786834328\n",
      "Epoch 59 completed. Loss: 0.1697086917631531\n",
      "Epoch 60 completed. Loss: 0.16958515781605626\n",
      "Epoch 61 completed. Loss: 0.16946670166047167\n",
      "Epoch 62 completed. Loss: 0.16931005388631096\n",
      "Epoch 63 completed. Loss: 0.16913761378168907\n",
      "Epoch 64 completed. Loss: 0.16893656803829135\n",
      "Epoch 65 completed. Loss: 0.1687217292439002\n",
      "Epoch 66 completed. Loss: 0.16851601824160622\n",
      "Epoch 67 completed. Loss: 0.1683227124214382\n",
      "Epoch 68 completed. Loss: 0.16814384741325478\n",
      "Epoch 69 completed. Loss: 0.167982074319775\n",
      "Epoch 70 completed. Loss: 0.16784231094572294\n",
      "Epoch 71 completed. Loss: 0.16772899289441826\n",
      "Epoch 72 completed. Loss: 0.1676422057846743\n",
      "Epoch 73 completed. Loss: 0.16758644057940822\n",
      "Epoch 74 completed. Loss: 0.1675367107925105\n",
      "Epoch 75 completed. Loss: 0.16746690660834307\n",
      "Epoch 76 completed. Loss: 0.16736674759938847\n",
      "Epoch 77 completed. Loss: 0.1672561032199476\n",
      "Epoch 78 completed. Loss: 0.1671559123023084\n",
      "Epoch 79 completed. Loss: 0.167068174129901\n",
      "Epoch 80 completed. Loss: 0.16700240165039076\n",
      "Epoch 81 completed. Loss: 0.16694911399765017\n",
      "Epoch 82 completed. Loss: 0.16690805211994647\n",
      "Epoch 83 completed. Loss: 0.16687346302556028\n",
      "Epoch 84 completed. Loss: 0.16685761755341\n",
      "Epoch 85 completed. Loss: 0.16684793925074023\n",
      "Epoch 86 completed. Loss: 0.1668310050140336\n",
      "Epoch 87 completed. Loss: 0.16680864114607988\n",
      "Epoch 88 completed. Loss: 0.16678963002855443\n",
      "Epoch 89 completed. Loss: 0.16676490360525753\n",
      "Epoch 90 completed. Loss: 0.1667456277303024\n",
      "Epoch 91 completed. Loss: 0.16672826036779773\n",
      "Epoch 92 completed. Loss: 0.1667139752501479\n",
      "Epoch 93 completed. Loss: 0.1667007119508727\n",
      "Epoch 94 completed. Loss: 0.16668214961275143\n",
      "Epoch 95 completed. Loss: 0.1666672350492722\n",
      "Epoch 96 completed. Loss: 0.16665502231260396\n",
      "Epoch 97 completed. Loss: 0.1666433132333188\n",
      "Epoch 98 completed. Loss: 0.1666293935899372\n",
      "Epoch 99 completed. Loss: 0.16661515722884837\n",
      "Epoch 100 completed. Loss: 0.1665976877715337\n",
      "Epoch 101 completed. Loss: 0.16657632004425915\n",
      "Epoch 102 completed. Loss: 0.16655471846307462\n",
      "Epoch 103 completed. Loss: 0.1665299935423133\n",
      "Epoch 104 completed. Loss: 0.16650557787059375\n",
      "Epoch 105 completed. Loss: 0.1664819203734505\n",
      "Epoch 106 completed. Loss: 0.16645516020328863\n",
      "Epoch 107 completed. Loss: 0.16642530138092873\n",
      "Epoch 108 completed. Loss: 0.16639683480395484\n",
      "Epoch 109 completed. Loss: 0.16636588883662431\n",
      "Epoch 110 completed. Loss: 0.16633158459683378\n",
      "Epoch 111 completed. Loss: 0.16629709775617496\n",
      "Epoch 112 completed. Loss: 0.16626246024215757\n",
      "Epoch 113 completed. Loss: 0.16622880764723372\n",
      "Epoch 114 completed. Loss: 0.16619324705929708\n",
      "Epoch 115 completed. Loss: 0.16615352016401172\n",
      "Epoch 116 completed. Loss: 0.1661121151409472\n",
      "Epoch 117 completed. Loss: 0.16607008475353008\n",
      "Epoch 118 completed. Loss: 0.1660294263799207\n",
      "Epoch 119 completed. Loss: 0.1659853124367078\n",
      "Epoch 120 completed. Loss: 0.1659421946480773\n",
      "Epoch 121 completed. Loss: 0.1659000749138137\n",
      "Epoch 122 completed. Loss: 0.1658533818195471\n",
      "Epoch 123 completed. Loss: 0.1658023422589248\n",
      "Epoch 124 completed. Loss: 0.16575527544605076\n",
      "Epoch 125 completed. Loss: 0.16570737757967477\n",
      "Epoch 126 completed. Loss: 0.16565976154977674\n",
      "Epoch 127 completed. Loss: 0.16561204994151163\n",
      "Epoch 128 completed. Loss: 0.16556333493725398\n",
      "Epoch 129 completed. Loss: 0.16552061286781214\n",
      "Epoch 130 completed. Loss: 0.1654798353855742\n",
      "Epoch 131 completed. Loss: 0.16543869201380362\n",
      "Epoch 132 completed. Loss: 0.16539868978655414\n",
      "Epoch 133 completed. Loss: 0.16535886756366638\n",
      "Epoch 134 completed. Loss: 0.165319201969023\n",
      "Epoch 135 completed. Loss: 0.16527798053336484\n",
      "Epoch 136 completed. Loss: 0.16523427910920693\n",
      "Epoch 137 completed. Loss: 0.1651960253555761\n",
      "Epoch 138 completed. Loss: 0.16515577017772903\n",
      "Epoch 139 completed. Loss: 0.16511154016817117\n",
      "Epoch 140 completed. Loss: 0.16506784540929578\n",
      "Epoch 141 completed. Loss: 0.16502456239690463\n",
      "Epoch 142 completed. Loss: 0.16498145104714304\n",
      "Epoch 143 completed. Loss: 0.1649351971894115\n",
      "Epoch 144 completed. Loss: 0.16488792095006605\n",
      "Epoch 145 completed. Loss: 0.16483538903670578\n",
      "Epoch 146 completed. Loss: 0.16478021868532164\n",
      "Epoch 147 completed. Loss: 0.16472191936471006\n",
      "Epoch 148 completed. Loss: 0.16465865498332205\n",
      "Epoch 149 completed. Loss: 0.1645928453070841\n",
      "Epoch 150 completed. Loss: 0.1645220672434197\n",
      "Epoch 151 completed. Loss: 0.16445260084181454\n",
      "Epoch 152 completed. Loss: 0.164382211723948\n",
      "Epoch 153 completed. Loss: 0.16431292861016236\n",
      "Epoch 154 completed. Loss: 0.16424411863374028\n",
      "Epoch 155 completed. Loss: 0.16417539314658108\n",
      "Epoch 156 completed. Loss: 0.16411150940368482\n",
      "Epoch 157 completed. Loss: 0.1640517073686749\n",
      "Epoch 158 completed. Loss: 0.1639933783462778\n",
      "Epoch 159 completed. Loss: 0.1639365732878663\n",
      "Epoch 160 completed. Loss: 0.16388113320309655\n",
      "Epoch 161 completed. Loss: 0.16382859366315616\n",
      "Epoch 162 completed. Loss: 0.16377660618436865\n",
      "Epoch 163 completed. Loss: 0.16372472218737213\n",
      "Epoch 164 completed. Loss: 0.1636748490793562\n",
      "Epoch 165 completed. Loss: 0.16362945907034923\n",
      "Epoch 166 completed. Loss: 0.16359036121940124\n",
      "Epoch 167 completed. Loss: 0.1635505778188065\n",
      "Epoch 168 completed. Loss: 0.1635116815641068\n",
      "Epoch 169 completed. Loss: 0.16347356129842755\n",
      "Epoch 170 completed. Loss: 0.16343540360973222\n",
      "Epoch 171 completed. Loss: 0.16339441418772674\n",
      "Epoch 172 completed. Loss: 0.16335437158102648\n",
      "Epoch 173 completed. Loss: 0.16331236143158895\n",
      "Epoch 174 completed. Loss: 0.1632627352138233\n",
      "Epoch 175 completed. Loss: 0.16320825475137488\n",
      "Epoch 176 completed. Loss: 0.16315327068561097\n",
      "Epoch 177 completed. Loss: 0.16309649150835517\n",
      "Epoch 178 completed. Loss: 0.16304026842463537\n",
      "Epoch 179 completed. Loss: 0.16297969227401185\n",
      "Epoch 180 completed. Loss: 0.1629214801805262\n",
      "Epoch 181 completed. Loss: 0.16286169466364692\n",
      "Epoch 182 completed. Loss: 0.1627973406459754\n",
      "Epoch 183 completed. Loss: 0.16273214095534402\n",
      "Epoch 184 completed. Loss: 0.1626606524336884\n",
      "Epoch 185 completed. Loss: 0.16258552974757326\n",
      "Epoch 186 completed. Loss: 0.16250498033886754\n",
      "Epoch 187 completed. Loss: 0.16240867346302398\n",
      "Epoch 188 completed. Loss: 0.16230468122931874\n",
      "Epoch 189 completed. Loss: 0.16221051648879176\n",
      "Epoch 190 completed. Loss: 0.16211673579065083\n",
      "Epoch 191 completed. Loss: 0.16203975133924714\n",
      "Epoch 192 completed. Loss: 0.16197142001277884\n",
      "Epoch 193 completed. Loss: 0.16191713665797067\n",
      "Epoch 194 completed. Loss: 0.1618741538150775\n",
      "Epoch 195 completed. Loss: 0.16183465498186583\n",
      "Epoch 196 completed. Loss: 0.16179671255342123\n",
      "Epoch 197 completed. Loss: 0.16175529850162226\n",
      "Epoch 198 completed. Loss: 0.16170307975486772\n",
      "Epoch 199 completed. Loss: 0.16162188278750284\n",
      "Epoch 200 completed. Loss: 0.16154313798205716\n",
      "Epoch 201 completed. Loss: 0.1614641829398777\n",
      "Epoch 202 completed. Loss: 0.16137965272111673\n",
      "Epoch 203 completed. Loss: 0.16131127873731987\n",
      "Epoch 204 completed. Loss: 0.16124491407772665\n",
      "Epoch 205 completed. Loss: 0.16113958291255331\n",
      "Epoch 206 completed. Loss: 0.16102937839006592\n",
      "Epoch 207 completed. Loss: 0.16092544575862514\n",
      "Epoch 208 completed. Loss: 0.16083563053902988\n",
      "Epoch 209 completed. Loss: 0.1607611553854477\n",
      "Epoch 210 completed. Loss: 0.16071335934537617\n",
      "Epoch 211 completed. Loss: 0.16068302764350625\n",
      "Epoch 212 completed. Loss: 0.16067855051117963\n",
      "Epoch 213 completed. Loss: 0.16069577588039932\n",
      "Epoch 214 completed. Loss: 0.16071978059655218\n",
      "Epoch 215 completed. Loss: 0.1607164889425496\n",
      "Epoch 216 completed. Loss: 0.16068425686058502\n",
      "Epoch 217 completed. Loss: 0.16068481441885835\n",
      "Epoch 218 completed. Loss: 0.1606867085838793\n",
      "Epoch 219 completed. Loss: 0.16067148767406877\n",
      "Epoch 220 completed. Loss: 0.1606412776079049\n",
      "Epoch 221 completed. Loss: 0.16058344526681553\n",
      "Epoch 222 completed. Loss: 0.16058464464726155\n",
      "Epoch 223 completed. Loss: 0.16055503622000541\n",
      "Epoch 224 completed. Loss: 0.1605073935275892\n",
      "Epoch 225 completed. Loss: 0.1604680020574832\n",
      "Epoch 226 completed. Loss: 0.16042754442950083\n",
      "Epoch 227 completed. Loss: 0.1603773536081124\n",
      "Epoch 228 completed. Loss: 0.16031140255474458\n",
      "Epoch 229 completed. Loss: 0.16025338970719738\n",
      "Epoch 230 completed. Loss: 0.16020513034031073\n",
      "Epoch 231 completed. Loss: 0.16015928768516757\n",
      "Epoch 232 completed. Loss: 0.160130856496584\n",
      "Epoch 233 completed. Loss: 0.1601411801067289\n",
      "Epoch 234 completed. Loss: 0.1602702805117794\n",
      "Epoch 235 completed. Loss: 0.16082938899576207\n",
      "Epoch 236 completed. Loss: 0.16314274250599842\n",
      "Epoch 237 completed. Loss: 0.16861305824523906\n",
      "Epoch 238 completed. Loss: 0.16956505175426265\n",
      "Epoch 239 completed. Loss: 0.1691448717536575\n",
      "Epoch 240 completed. Loss: 0.1676439351666032\n",
      "Epoch 241 completed. Loss: 0.16494806193515868\n",
      "Epoch 242 completed. Loss: 0.1634953615983344\n",
      "Epoch 243 completed. Loss: 0.16263169089490642\n",
      "Epoch 244 completed. Loss: 0.16203119881152103\n",
      "Epoch 245 completed. Loss: 0.1615880596810701\n",
      "Epoch 246 completed. Loss: 0.161192028864498\n",
      "Epoch 247 completed. Loss: 0.16084334473100806\n",
      "Epoch 248 completed. Loss: 0.1605460293423168\n",
      "Epoch 249 completed. Loss: 0.16028618891436494\n",
      "Epoch 250 completed. Loss: 0.1600509182171707\n",
      "Epoch 251 completed. Loss: 0.15983563537442463\n",
      "Epoch 252 completed. Loss: 0.15962673369493557\n",
      "Epoch 253 completed. Loss: 0.15942836405766445\n",
      "Epoch 254 completed. Loss: 0.15923734435733697\n",
      "Epoch 255 completed. Loss: 0.1590533223038841\n",
      "Epoch 256 completed. Loss: 0.15887167541058247\n",
      "Epoch 257 completed. Loss: 0.15869790078711227\n",
      "Epoch 258 completed. Loss: 0.15854773788172752\n",
      "Epoch 259 completed. Loss: 0.15838036764752486\n",
      "Epoch 260 completed. Loss: 0.15819008882485466\n",
      "Epoch 261 completed. Loss: 0.1580154451945014\n",
      "Epoch 262 completed. Loss: 0.1578815786342038\n",
      "Epoch 263 completed. Loss: 0.15783401066770064\n",
      "Epoch 264 completed. Loss: 0.1581173673703306\n",
      "Epoch 265 completed. Loss: 0.15833305503604783\n",
      "Epoch 266 completed. Loss: 0.1581409832337489\n",
      "Epoch 267 completed. Loss: 0.15833614543928734\n",
      "Epoch 268 completed. Loss: 0.15825559444774734\n",
      "Epoch 269 completed. Loss: 0.15811880381543783\n",
      "Epoch 270 completed. Loss: 0.15792145865904023\n",
      "Epoch 271 completed. Loss: 0.15750397964690496\n",
      "Epoch 272 completed. Loss: 0.1571091804357812\n",
      "Epoch 273 completed. Loss: 0.1573555349318446\n",
      "Epoch 274 completed. Loss: 0.1580356511092537\n",
      "Epoch 275 completed. Loss: 0.1598476845191886\n",
      "Epoch 276 completed. Loss: 0.1624405233604414\n",
      "Epoch 277 completed. Loss: 0.1639456865397245\n",
      "Epoch 278 completed. Loss: 0.16501468202664654\n",
      "Epoch 279 completed. Loss: 0.1660141123916964\n",
      "Epoch 280 completed. Loss: 0.16688806278061508\n",
      "Epoch 281 completed. Loss: 0.16767379875917302\n",
      "Epoch 282 completed. Loss: 0.16827853599284795\n",
      "Epoch 283 completed. Loss: 0.16872040236090702\n",
      "Epoch 284 completed. Loss: 0.16908508450893248\n",
      "Epoch 285 completed. Loss: 0.16935804377535094\n",
      "Epoch 286 completed. Loss: 0.16967347951469616\n",
      "Epoch 287 completed. Loss: 0.1699847160743473\n",
      "Epoch 288 completed. Loss: 0.1703418422823834\n",
      "Epoch 289 completed. Loss: 0.170886107070338\n",
      "Epoch 290 completed. Loss: 0.17176314099321102\n",
      "Epoch 291 completed. Loss: 0.17161809439617848\n",
      "Epoch 292 completed. Loss: 0.1715826431144918\n",
      "Epoch 293 completed. Loss: 0.17160489038865054\n",
      "Epoch 294 completed. Loss: 0.1717485831481135\n",
      "Epoch 295 completed. Loss: 0.1746450392328095\n",
      "Epoch 296 completed. Loss: 0.20843464356264896\n",
      "Epoch 297 completed. Loss: 0.20757156243060676\n",
      "Epoch 298 completed. Loss: 0.205141409826255\n",
      "Epoch 299 completed. Loss: 0.20170700170831846\n",
      "Epoch 300 completed. Loss: 0.19926899263374237\n",
      "Epoch 301 completed. Loss: 0.20317723864302242\n",
      "Epoch 302 completed. Loss: 0.23044001459434774\n",
      "Epoch 303 completed. Loss: 0.275109937981044\n",
      "Epoch 304 completed. Loss: 0.2525825962875592\n",
      "Epoch 305 completed. Loss: 0.2576121770840961\n",
      "Epoch 306 completed. Loss: 0.2389003706679345\n",
      "Epoch 307 completed. Loss: 0.23038146941059479\n",
      "Epoch 308 completed. Loss: 0.2258978712042176\n",
      "Epoch 309 completed. Loss: 0.22260160883036756\n",
      "Epoch 310 completed. Loss: 0.22022572958555622\n",
      "Epoch 311 completed. Loss: 0.21862134396380972\n",
      "Epoch 312 completed. Loss: 0.21767571275201597\n",
      "Epoch 313 completed. Loss: 0.21707065326285985\n",
      "Epoch 314 completed. Loss: 0.21665600034756458\n",
      "Epoch 315 completed. Loss: 0.21650049787013295\n",
      "Epoch 316 completed. Loss: 0.22333033870834612\n",
      "Epoch 317 completed. Loss: 0.24840745879570414\n",
      "Epoch 318 completed. Loss: 0.2821010081436629\n",
      "Epoch 319 completed. Loss: 0.2963561929489595\n",
      "Epoch 320 completed. Loss: 0.3015919032918358\n",
      "Epoch 321 completed. Loss: 0.3054977634031659\n",
      "Epoch 322 completed. Loss: 0.31701517782134636\n",
      "Epoch 323 completed. Loss: 0.3211215771004773\n",
      "Epoch 324 completed. Loss: 0.27866847834774283\n",
      "Epoch 325 completed. Loss: 0.24751844834179013\n",
      "Epoch 326 completed. Loss: 0.22610502823895837\n",
      "Epoch 327 completed. Loss: 0.21577697053916184\n",
      "Epoch 328 completed. Loss: 0.20863978530807822\n",
      "Epoch 329 completed. Loss: 0.2071009370618737\n",
      "Epoch 330 completed. Loss: 0.2008678502379917\n",
      "Epoch 331 completed. Loss: 0.19306109854386805\n",
      "Epoch 332 completed. Loss: 0.19148090700393736\n",
      "Epoch 333 completed. Loss: 0.1899041234730645\n",
      "Epoch 334 completed. Loss: 0.18719697261279422\n",
      "Epoch 335 completed. Loss: 0.18365620881301992\n",
      "Epoch 336 completed. Loss: 0.17831233549471728\n",
      "Epoch 337 completed. Loss: 0.17470683494202272\n",
      "Epoch 338 completed. Loss: 0.1727226821511671\n",
      "Epoch 339 completed. Loss: 0.17099468359554704\n",
      "Epoch 340 completed. Loss: 0.16946038084637274\n",
      "Epoch 341 completed. Loss: 0.16808485844459223\n",
      "Epoch 342 completed. Loss: 0.16700661219478788\n",
      "Epoch 343 completed. Loss: 0.16601535323056335\n",
      "Epoch 344 completed. Loss: 0.1651197021535825\n",
      "Epoch 345 completed. Loss: 0.1643210245049464\n",
      "Epoch 346 completed. Loss: 0.1636176044109505\n",
      "Epoch 347 completed. Loss: 0.16300925650868817\n",
      "Epoch 348 completed. Loss: 0.1624969225346925\n",
      "Epoch 349 completed. Loss: 0.16207904370980694\n",
      "Epoch 350 completed. Loss: 0.16174997699475785\n",
      "Epoch 351 completed. Loss: 0.16150078792383843\n",
      "Epoch 352 completed. Loss: 0.16132077105444256\n",
      "Epoch 353 completed. Loss: 0.16119875691497235\n",
      "Epoch 354 completed. Loss: 0.161123983952855\n",
      "Epoch 355 completed. Loss: 0.16108658958640193\n",
      "Epoch 356 completed. Loss: 0.16107783094563033\n",
      "Epoch 357 completed. Loss: 0.16109013534909464\n",
      "Epoch 358 completed. Loss: 0.16111705782065597\n",
      "Epoch 359 completed. Loss: 0.1611532018548147\n",
      "Epoch 360 completed. Loss: 0.16119414034299376\n",
      "Epoch 361 completed. Loss: 0.16123635330844982\n",
      "Epoch 362 completed. Loss: 0.1612771782280231\n",
      "Epoch 363 completed. Loss: 0.16131475351254307\n",
      "Epoch 364 completed. Loss: 0.16134793433182917\n",
      "Epoch 365 completed. Loss: 0.16137617329611487\n",
      "Epoch 366 completed. Loss: 0.1613993766496647\n",
      "Epoch 367 completed. Loss: 0.16141775725578825\n",
      "Epoch 368 completed. Loss: 0.16143170445014798\n",
      "Epoch 369 completed. Loss: 0.16144168235521786\n",
      "Epoch 370 completed. Loss: 0.16144815917712094\n",
      "Epoch 371 completed. Loss: 0.1614515640327491\n",
      "Epoch 372 completed. Loss: 0.16145226528907655\n",
      "Epoch 373 completed. Loss: 0.1614505640612655\n",
      "Epoch 374 completed. Loss: 0.1614466972563491\n",
      "Epoch 375 completed. Loss: 0.16144084569828032\n",
      "Epoch 376 completed. Loss: 0.1614331805532189\n",
      "Epoch 377 completed. Loss: 0.16140917009466607\n",
      "Epoch 378 completed. Loss: 0.16138773187128644\n",
      "Epoch 379 completed. Loss: 0.16135001410924785\n",
      "Epoch 380 completed. Loss: 0.1612939306415845\n",
      "Epoch 381 completed. Loss: 0.16123896743694188\n",
      "Epoch 382 completed. Loss: 0.16119218032622126\n",
      "Epoch 383 completed. Loss: 0.16112949589725503\n",
      "Epoch 384 completed. Loss: 0.16105798232625054\n",
      "Epoch 385 completed. Loss: 0.16100091999107397\n",
      "Epoch 386 completed. Loss: 0.16094142497018016\n",
      "Epoch 387 completed. Loss: 0.16088827864281594\n",
      "Epoch 388 completed. Loss: 0.1608404934908729\n",
      "Epoch 389 completed. Loss: 0.16079825803345235\n",
      "Epoch 390 completed. Loss: 0.16075948409521784\n",
      "Epoch 391 completed. Loss: 0.1607238175094297\n",
      "Epoch 392 completed. Loss: 0.16069096561759472\n",
      "Epoch 393 completed. Loss: 0.16066068579159717\n",
      "Epoch 394 completed. Loss: 0.1606327770862759\n",
      "Epoch 395 completed. Loss: 0.16060707345552155\n",
      "Epoch 396 completed. Loss: 0.16058343820893128\n",
      "Epoch 397 completed. Loss: 0.16056175945736226\n",
      "Epoch 398 completed. Loss: 0.1605419463502504\n",
      "Epoch 399 completed. Loss: 0.16052392594940582\n",
      "Epoch 400 completed. Loss: 0.1605076406163749\n",
      "Epoch 401 completed. Loss: 0.1604930458126786\n",
      "Epoch 402 completed. Loss: 0.16048010821861572\n",
      "Epoch 403 completed. Loss: 0.1604688032307845\n",
      "Epoch 404 completed. Loss: 0.16045911464353046\n",
      "Epoch 405 completed. Loss: 0.16045103450196363\n",
      "Epoch 406 completed. Loss: 0.16044456098369483\n",
      "Epoch 407 completed. Loss: 0.16043969743217829\n",
      "Epoch 408 completed. Loss: 0.160436451459166\n",
      "Epoch 409 completed. Loss: 0.16043483426141303\n",
      "Epoch 410 completed. Loss: 0.1604348611853477\n",
      "Epoch 411 completed. Loss: 0.16043654826638784\n",
      "Epoch 412 completed. Loss: 0.16043991203165467\n",
      "Epoch 413 completed. Loss: 0.16044497270566624\n",
      "Epoch 414 completed. Loss: 0.16045175127737107\n",
      "Epoch 415 completed. Loss: 0.16046026895277846\n",
      "Epoch 416 completed. Loss: 0.1604705466916856\n",
      "Epoch 417 completed. Loss: 0.16048260475941184\n",
      "Epoch 418 completed. Loss: 0.16049646229158104\n",
      "Epoch 419 completed. Loss: 0.1605121368712382\n",
      "Epoch 420 completed. Loss: 0.1605296441184051\n",
      "Epoch 421 completed. Loss: 0.16054899729286184\n",
      "Epoch 422 completed. Loss: 0.16057020691158175\n",
      "Epoch 423 completed. Loss: 0.16059328038273168\n",
      "Epoch 424 completed. Loss: 0.16061822165855497\n",
      "Epoch 425 completed. Loss: 0.1606450309096795\n",
      "Epoch 426 completed. Loss: 0.16067370422349272\n",
      "Epoch 427 completed. Loss: 0.16070423332913217\n",
      "Epoch 428 completed. Loss: 0.16073660535134146\n",
      "Epoch 429 completed. Loss: 0.16077080259495055\n",
      "Epoch 430 completed. Loss: 0.16080680236102396\n",
      "Epoch 431 completed. Loss: 0.16084457679478545\n",
      "Epoch 432 completed. Loss: 0.16088409276430127\n",
      "Epoch 433 completed. Loss: 0.16092531176759672\n",
      "Epoch 434 completed. Loss: 0.16096818986445002\n",
      "Epoch 435 completed. Loss: 0.16101267762757226\n",
      "Epoch 436 completed. Loss: 0.16105872010636707\n",
      "Epoch 437 completed. Loss: 0.16110625679497081\n",
      "Epoch 438 completed. Loss: 0.16115522159494428\n",
      "Epoch 439 completed. Loss: 0.16120554276185092\n",
      "Epoch 440 completed. Loss: 0.1612571428241183\n",
      "Epoch 441 completed. Loss: 0.16130993846207387\n",
      "Epoch 442 completed. Loss: 0.16136384033493015\n",
      "Epoch 443 completed. Loss: 0.16141875284376528\n",
      "Epoch 444 completed. Loss: 0.16147457381922203\n",
      "Epoch 445 completed. Loss: 0.16153119412369993\n",
      "Epoch 446 completed. Loss: 0.16158849715919404\n",
      "Epoch 447 completed. Loss: 0.16164635827359153\n",
      "Epoch 448 completed. Loss: 0.1617046440601795\n",
      "Epoch 449 completed. Loss: 0.16176321154727866\n",
      "Epoch 450 completed. Loss: 0.16182190727740503\n",
      "Epoch 451 completed. Loss: 0.16188056627828257\n",
      "Epoch 452 completed. Loss: 0.16193901093168805\n",
      "Epoch 453 completed. Loss: 0.16199704975092197\n",
      "Epoch 454 completed. Loss: 0.16205447608435727\n",
      "Epoch 455 completed. Loss: 0.16211106677199522\n",
      "Epoch 456 completed. Loss: 0.16216658079544963\n",
      "Epoch 457 completed. Loss: 0.1622207579808935\n",
      "Epoch 458 completed. Loss: 0.16227331784088767\n",
      "Epoch 459 completed. Loss: 0.16232395867622018\n",
      "Epoch 460 completed. Loss: 0.16237235710371403\n",
      "Epoch 461 completed. Loss: 0.16241816822930163\n",
      "Epoch 462 completed. Loss: 0.162461026742968\n",
      "Epoch 463 completed. Loss: 0.16250054926355845\n",
      "Epoch 464 completed. Loss: 0.16253633828985428\n",
      "Epoch 465 completed. Loss: 0.16256798809589126\n",
      "Epoch 466 completed. Loss: 0.1625950928165975\n",
      "Epoch 467 completed. Loss: 0.16261725678546607\n",
      "Epoch 468 completed. Loss: 0.16263410691379246\n",
      "Epoch 469 completed. Loss: 0.16264530658483928\n",
      "Epoch 470 completed. Loss: 0.1626505702624078\n",
      "Epoch 471 completed. Loss: 0.16264967788871784\n",
      "Epoch 472 completed. Loss: 0.162642488253004\n",
      "Epoch 473 completed. Loss: 0.16262895085694884\n",
      "Epoch 474 completed. Loss: 0.16260911630040245\n",
      "Epoch 475 completed. Loss: 0.16258314577305735\n",
      "Epoch 476 completed. Loss: 0.1625513206886214\n",
      "Epoch 477 completed. Loss: 0.16251405096251462\n",
      "Epoch 478 completed. Loss: 0.1624718918697713\n",
      "Epoch 479 completed. Loss: 0.16242555306959455\n",
      "Epoch 480 completed. Loss: 0.16237592002645296\n",
      "Epoch 481 completed. Loss: 0.1623240734989078\n",
      "Epoch 482 completed. Loss: 0.16227130949960442\n",
      "Epoch 483 completed. Loss: 0.16221915611238635\n",
      "Epoch 484 completed. Loss: 0.16216938387853175\n",
      "Epoch 485 completed. Loss: 0.162124001148102\n",
      "Epoch 486 completed. Loss: 0.16208522480546358\n",
      "Epoch 487 completed. Loss: 0.16205541458610606\n",
      "Epoch 488 completed. Loss: 0.16203695895910866\n",
      "Epoch 489 completed. Loss: 0.1620321037983774\n",
      "Epoch 490 completed. Loss: 0.1620427236343091\n",
      "Epoch 491 completed. Loss: 0.16207005014865755\n",
      "Epoch 492 completed. Loss: 0.16211439238666236\n",
      "Epoch 493 completed. Loss: 0.16217490284748384\n",
      "Epoch 494 completed. Loss: 0.16224945450516354\n",
      "Epoch 495 completed. Loss: 0.16233468649752542\n",
      "Epoch 496 completed. Loss: 0.1624262460181896\n",
      "Epoch 497 completed. Loss: 0.16251920612275295\n",
      "Epoch 498 completed. Loss: 0.16260858946289022\n",
      "Epoch 499 completed. Loss: 0.16268989617316798\n",
      "Epoch 500 completed. Loss: 0.16275953366128004\n",
      "Fit done. \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predictions...\n",
      "Predictions done. \n",
      "\n",
      "{'Accuracy': 0.8052434456928839, 'Precision': 0.735632183908046, 'Recall': 0.6881720430107527, 'F1 Score': 0.7111111111111111}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ANN()\n",
    "\n",
    "model.add(layers.layer(24, 'ReLU'))\n",
    "model.add(layers.layer(24, 'sigmoid'))\n",
    "model.add(layers.layer(6, 'ReLU'))\n",
    "# model.add(layers.layer(4, 'ReLU'))\n",
    "model.add(layers.layer(1, 'sigmoid'))\n",
    "model.set_loss_function('mse')\n",
    "model.set_learning_rate(0.1)\n",
    "\n",
    "num_epochs = 500\n",
    "model.Fit(X_train, Y_train, num_epochs)\n",
    "el = model.get_losses()\n",
    "# acc_avg_val = model.get_avg_accuracy()\n",
    "# print()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(np.arange(1, num_epochs+1), el, label='mu')\n",
    "plt.title('Avg Loss by epoch', fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix1(Y_test, predictions)\n",
    "print(calculate_metrics(Y_test, predictions))\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\n",
    "plt.figure(figsize = (7,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "plt.xlabel(\"Predicted Class\", fontsize=18)\n",
    "plt.ylabel(\"True Class\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission to test model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-15T09:33:50.778619Z",
     "iopub.status.busy": "2024-05-15T09:33:50.778319Z",
     "iopub.status.idle": "2024-05-15T09:33:50.838773Z",
     "shell.execute_reply": "2024-05-15T09:33:50.837483Z",
     "shell.execute_reply.started": "2024-05-15T09:33:50.778576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predictions...\n",
      "Predictions done. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='submission.csv' target='_blank'>submission.csv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/submission.csv"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "test_data.head(4)\n",
    "\n",
    "# We apply the dictionary using a lambda function and the pandas .apply() module\n",
    "test_data['Bsex'] = test_data['Sex'].apply(lambda x : dict_sex[x])\n",
    "\n",
    "\n",
    "X_t = test_data[['Pclass', 'Bsex']].to_numpy()\n",
    "\n",
    "test_predictions = model.predict(X_t)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data[\"PassengerId\"],\n",
    "        \"Survived\": test_predictions\n",
    "    })\n",
    "\n",
    "\n",
    "submission['Survived'] = submission['Survived'].astype(int)\n",
    "# Export it in a 'Comma Separated Values' (CSV) file\n",
    "import os\n",
    "os.chdir(r'../working')\n",
    "submission.to_csv(r'submission.csv', index=False)\n",
    "# Creating a link to download the .csv file we created\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'submission.csv')\n",
    "# submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 29844,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
